{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bd9276-895f-4af7-8d72-a6d90ef04a64",
   "metadata": {},
   "source": [
    "# Basin Averaged AORC Precipitation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c3f86-cc14-40c1-a977-e2a443a086f9",
   "metadata": {},
   "source": [
    "Last update: \n",
    "August 7, 2023\n",
    "\n",
    "Author(s): \n",
    "- Irene Garousi-Nejad: igarousi@cuahsi.org\n",
    "- Tony Castronova acastronova@cuahsi.org\n",
    "\n",
    "**Description**:\n",
    "This script retrieves and calculates the basin avergare precipitation data from the AORC (Analysis of Record for Calibration) products, used for the NWM retrospective version 2.x, within a specified area and time period of interest. Note that this script is used to process a single month of hourly data. Attempting to use it for periods longer than a month could lead to time inefficiencies and potential code breakdowns.\n",
    "\n",
    "**Data Links**: \n",
    "- Original: https://noaa-nwm-retrospective-2-1-pds.s3.amazonaws.com/index.html#forcing/\n",
    "- Kerchunk: https://ciroh-nwm-zarr-retrospective-data-copy.s3.amazonaws.com/index.html#noaa-nwm-retrospective-2-1-zarr-pds/\n",
    "- Description: https://hydrology.nws.noaa.gov/aorc-historic/Documents/AORC-Version1.1-SourcesMethodsandVerifications.pdf\n",
    "\n",
    "**Software Requirements**: TODO\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab978fe-7ba9-4bb4-8749-1ca97e0de98b",
   "metadata": {},
   "source": [
    "### Install and Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e667a7a-5ebc-49c0-b216-7cece3f26701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install s3fs  --quiet\n",
    "!pip install kerchunk --quiet\n",
    "!pip install zarr --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install rioxarray --quiet\n",
    "!pip install geocube --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcbacd-cf89-4268-98af-5ec306c9dff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import dask\n",
    "import numpy\n",
    "import xarray\n",
    "import pyproj\n",
    "import pandas\n",
    "import requests\n",
    "import geopandas\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.distributed import progress\n",
    "import zarr\n",
    "import fsspec\n",
    "from pyproj import Transformer\n",
    "from s3fs import S3FileSystem\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import rioxarray\n",
    "import geocube\n",
    "import pandas as pd\n",
    "from geocube.api.core import make_geocube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd93c6-c49a-4f31-9dff-d98698d53f67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot the geospatial model domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d19df-d905-4ea6-8c3b-2c2b812912f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the shapefile\n",
    "gdf = geopandas.read_file(f'./GISBasins/WeberRiverBasin.shp')\n",
    "gdf.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fae731-ae9d-4929-bbc1-698b3e2deedd",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc683339",
   "metadata": {},
   "source": [
    "Please note that this jupyter notebook works for data within 2007-2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767431ba-ea78-4046-bc38-ac933c71c589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select a year of interest\n",
    "year = '2015'\n",
    "month_s='04'\n",
    "month_e='05'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bb2dc-903a-4484-aa94-5a3c4f39b7d3",
   "metadata": {},
   "source": [
    "### Load Forcing Data into Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e27c5",
   "metadata": {},
   "source": [
    "These data are publicly available for the entire CONUS, spanning from 1980 to 2020. Kerchunk header files have been created by the Alabama Water Institute team and this is an ongoing project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cbc7b-f303-4f3f-add4-d8c7a2924325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = 's3://ciroh-nwm-zarr-retrospective-data-copy/noaa-nwm-retrospective-2-1-zarr-pds/forcing/'\n",
    "\n",
    "# create an instace of the S3FileSystem class from s3fs\n",
    "s3 = S3FileSystem(anon=True)\n",
    "files = s3.ls(f'{bucket}{year}')  \n",
    "\n",
    "new_files = []\n",
    "for f in files:\n",
    "    parts = f.split('/')\n",
    "    parts[0] += '.s3.amazonaws.com'\n",
    "    parts.insert(0, 'https:/')\n",
    "    new_name = '/'.join(parts)\n",
    "    new_files.append(new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c70fe4",
   "metadata": {},
   "source": [
    "Considering the memory limitations, it is necessary to choose a smaller subset of the dataset. Afterwards, we can utilize the MultiZarrToZarr function from the kerchunk library to merge the individual header files and generate a single kerchunk file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec19b9-61b3-4594-ab54-3d48c5ebd28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# select a smaller chunck of kerchunk files \n",
    "if month_s in ['01','02','03']:\n",
    "    json_list = new_files[0*2190:2190] \n",
    "elif month_s in ['04','05','06']:\n",
    "    json_list = new_files[2190-(4*24):2190*2]\n",
    "elif month_s in ['07','08','09']:\n",
    "    json_list = new_files[2190*2-(5*24):2190*3]\n",
    "elif month_s in ['10','11','12']:\n",
    "    json_list = new_files[2190*3-(5*24):]\n",
    "\n",
    "mzz = MultiZarrToZarr(json_list,\n",
    "    remote_protocol='s3',\n",
    "    remote_options={'anon':True},\n",
    "    concat_dims=['valid_time'])\n",
    "\n",
    "d = mzz.translate()\n",
    "\n",
    "backend_args = {\"consolidated\": False, \"storage_options\": {\"fo\": d}, \"consolidated\": False}\n",
    "\n",
    "ds = xarray.open_dataset(\"reference://\", engine=\"zarr\", backend_kwargs=backend_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5343d-ff72-4adb-b28d-93be9722974b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14687e0-73a1-4d16-99d4-0cd7e5910b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove the dimension with the size of 1\n",
    "ds = ds.squeeze(dim='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f506b5-214f-4023-b15a-93fcc4b9052b",
   "metadata": {},
   "source": [
    "### Add spatial metadata to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb6b33",
   "metadata": {},
   "source": [
    "Load the National Water Model metadata dataset using xarray and add spatial metadata to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa823fca-f98d-4990-bf09-127ec8c9a780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_meta = xarray.open_dataset('http://thredds.hydroshare.org/thredds/dodsC/hydroshare/resources/2a8a3566e1c84b8eb3871f30841a3855/data/contents/WRF_Hydro_NWM_geospatial_data_template_land_GIS.nc')\n",
    "\n",
    "leny = len(ds_meta.y)\n",
    "x = ds_meta.x.values\n",
    "y = ds_meta.y.values\n",
    "\n",
    "ds = ds.rename({'valid_time': 'time', 'south_north':'y', 'west_east':'x'})\n",
    "\n",
    "X, Y = numpy.meshgrid(x, y)\n",
    "\n",
    "# define the input crs\n",
    "wrf_proj = pyproj.Proj(proj='lcc',\n",
    "                       lat_1=30.,\n",
    "                       lat_2=60., \n",
    "                       lat_0=40.0000076293945, lon_0=-97., # Center point\n",
    "                       a=6370000, b=6370000)\n",
    "\n",
    "# define the output crs\n",
    "wgs_proj = pyproj.Proj(proj='latlong', datum='WGS84')\n",
    "\n",
    "# transform X, Y into Lat, Lon\n",
    "transformer = pyproj.Transformer.from_crs(wrf_proj.crs, wgs_proj.crs)\n",
    "lon, lat = transformer.transform(X, Y)\n",
    "\n",
    "ds = ds.assign_coords(lon = (['y', 'x'], lon))\n",
    "ds = ds.assign_coords(lat = (['y', 'x'], lat))\n",
    "ds = ds.assign_coords(x = x)\n",
    "ds = ds.assign_coords(y = y)\n",
    "\n",
    "ds.x.attrs['axis'] = 'X'\n",
    "ds.x.attrs['standard_name'] = 'projection_x_coordinate'\n",
    "ds.x.attrs['long_name'] = 'x-coordinate in projected coordinate system'\n",
    "ds.x.attrs['resolution'] = 1000.  # cell size\n",
    "\n",
    "ds.y.attrs['axis'] = 'Y' \n",
    "ds.y.attrs['standard_name'] = 'projection_y_coordinate'\n",
    "ds.y.attrs['long_name'] = 'y-coordinate in projected coordinate system'\n",
    "ds.y.attrs['resolution'] = 1000.  # cell size\n",
    "\n",
    "ds.lon.attrs['units'] = 'degrees_east'\n",
    "ds.lon.attrs['standard_name'] = 'longitude' \n",
    "ds.lon.attrs['long_name'] = 'longitude'\n",
    "\n",
    "ds.lat.attrs['units'] = 'degrees_north'\n",
    "ds.lat.attrs['standard_name'] = 'latitude' \n",
    "ds.lat.attrs['long_name'] = 'latitude'\n",
    "\n",
    "# add crs to netcdf file\n",
    "ds.rio.write_crs(ds_meta.crs.attrs['spatial_ref'], inplace=True\n",
    "                ).rio.set_spatial_dims(x_dim=\"x\",\n",
    "                                       y_dim=\"y\",\n",
    "                                       inplace=True,\n",
    "                                       ).rio.write_coordinate_system(inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0328b-7d24-4c18-9ef8-515399cb6639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b39d7-3e17-4a81-808b-b06a4f51ff3a",
   "metadata": {},
   "source": [
    "### Add spatial reference to the model domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1c252-7cbf-4fde-ada3-222afbdc69b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert domain geometry data into the projection of our forcing data\n",
    "target_crs = pyproj.Proj(proj='lcc',\n",
    "                       lat_1=30.,\n",
    "                       lat_2=60., \n",
    "                       lat_0=40.0000076293945, lon_0=-97., # Center point\n",
    "                       a=6370000, b=6370000) \n",
    "\n",
    "gdf = gdf.to_crs(target_crs.crs)\n",
    "\n",
    "gdf['geometry'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a59cc-2f7a-47ff-9e62-73261b5c8b75",
   "metadata": {},
   "source": [
    "Rechunk the dataset before the next steps to ensure we do not get any memory limit issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b2282-0810-4480-a275-8f89029f8156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# important step\n",
    "# rechunk the dataset to solve the memory limit issue\n",
    "ds = ds.chunk(chunks={'time': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534983b7-189b-4ea9-ad73-c4e8ae40e34c",
   "metadata": {},
   "source": [
    "### Clip the CONUS-wide AORC to the extent of the model domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a540508",
   "metadata": {},
   "source": [
    "Add catchment ids to the geodataset. These will be used to perform zonal statistics later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13f761-b9a2-4618-8e73-dfcc9955da27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create zonal id column\n",
    "gdf['cat'] = gdf.HUC8.astype(int)\n",
    "\n",
    "# clip AORC to the extent of the hydrofabric geometries\n",
    "ds = ds.rio.clip(gdf.geometry.values,\n",
    "                 gdf.crs,\n",
    "                 drop=True,\n",
    "                 invert=False, from_disk=True)\n",
    "\n",
    "# create a grid for the geocube\n",
    "out_grid = make_geocube(\n",
    "    vector_data=gdf,\n",
    "    measurements=[\"cat\"],\n",
    "    like=ds # ensure the data are on the same grid\n",
    ")\n",
    "\n",
    "# add the catchment variable to the original dataset\n",
    "ds = ds.assign_coords(cat = (['y','x'], out_grid.cat.data))\n",
    "\n",
    "# compute the unique catchment IDs which will be used to compute zonal statistics\n",
    "catchment_ids = numpy.unique(ds.cat.data[~numpy.isnan(ds.cat.data)])\n",
    "\n",
    "print(f'The dataset contains {len(catchment_ids)} catchments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b3810-a62d-462d-9cf3-825537d36f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5a5a2-a4ca-4167-99b5-83cb62e450c8",
   "metadata": {},
   "source": [
    "### Preview the gridded catchments over the watershed vector boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70924bd-2226-4927-9cf3-811491132afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "# plot the gridded catchment mapping\n",
    "ds.cat.plot()\n",
    "\n",
    "# preview map geometries\n",
    "gdf.iloc[:].plot(ax=ax, linewidth=2, edgecolor='k', facecolor='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14020f",
   "metadata": {},
   "source": [
    "### Run the main functions that calculate basin average precipitation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f7a62",
   "metadata": {},
   "source": [
    "Initiate the Dask client. This will enable us to parallelize our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=6,\n",
    "                       memory_limit='2GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e0dba",
   "metadata": {},
   "source": [
    "First, import the `main_compute_updt.py` as a module. Then, invoke the `compute_avg_p` fucntion from this module to perform the necessary calculations. Note that the method we're using will associate grid cell with the watershed that it overlaps the most with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5dd522-d7eb-4185-9f9f-e4d43ded2dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import main_compute_updt\n",
    "\n",
    "main_compute_updt.compute_avg_p(client, ds, catchment_ids, year, month_s, month_e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:iguide]",
   "language": "python",
   "name": "conda-env-iguide-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
